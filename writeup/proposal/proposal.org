#+TITLE: CS224n Project Proposal: Word2Bits
#+LATEX_HEADER: \usepackage[a4paper,margin=3cm,footskip=.5cm]{geometry}
#+LATEX_HEADER: \usepackage{listings}
#+LaTeX_CLASS_OPTIONS: [microtype]
#+LaTeX_CLASS_OPTIONS: [10pt]
#+LATEX_HEADER: \usepackage{amsmath}
#+OPTIONS: toc:nil
#+AUTHOR: Maximilian Lam
#+DATE:

* Problem
  Word embeddings are often high dimensional (200-300 parameters) and
  high precision (32-bit floating point numbers), and thus take up
  lots of memory and space; this poses a problem to mobile devices
  running NLP models as they have low network bandwidth for downloading
  word embeddings and limited memory and storage for processing them.
  \\
  \\
  We propose learning quantized word embeddings such that each
  parameter takes less than 4 bits (1 bit would be optimal). We aim to
  show that despite drastically reducing the precision of the
  parameters we can still achieve state of the art or near state of
  the art accuracy on analogy tasks, and NLP machine learning / deep
  learning applications.
  \\
  \\
  Furthermore, we will investigate the tradeoffs between very low precision
  embeddings and high precision embeddings -- e.g: effects on training
  time, accuracy, embedding dimensionality, performance, etc.
  \\
  \\
  Finally, we aim to show that using quantized embeddings we can
  drastically reduce our memory and storage footprint through
  compression -- and if we have time to show that very low precision
  embeddings can help interpretability.
  \\
  \\
  Our main contribution in this project is showing that high
  dimensional, low precision word embeddings (bit vectors) have the
  capacity to capture relationships between words -- a task that is
  thought to require high precision parameters -- and that doing so
  drastically reduces the memory and storage they take.

* Data
  There is lots of data for training word embeddings. Specifically we will use
  - Training: Wikipedia corpus (100 mb, 1 gb+)
  - Testing: Google analogy test set (as used by Mikolov)
  - Semi-reach task data : Sentiment analysis data (e.g: IMDB)
  - Reach task data : Language translation pairs

* Algorithm
  To train low precision embeddings, we will use an algorithm similar
  to that of https://arxiv.org/abs/1602.02830 (binarized neural
  networks), which uses Hinton's straight-through estimator to
  evaluate the gradient of discrete functions.
  \\
  \\
  To determine the boundaries for floating point quantization, we will
  start with hand picked values. A potential idea is to learn the
  boundaries that yield better results, and possibly determine which
  parameters to quantize.
  \\
  \\
  For compression, we will greedily sort rows and columns to maximize
  consecutive parameters that have the same value.

* Evaluation Plan
  - We will evaluate our results using the google analogy test set.
    - We expect a tradeoff between required dimensionality and training time (epochs).
      - Specifically, we expect to reach state of the art accuracy on
	the analogy task with higher word embedding dimensionality and
	more training.
    - We expect that highly quantized embeddings will have an optima
      at a higher vector dimensionality than full precision embeddings.
  - Semi-Reach: Utilize bit word embeddings in sentiment analysis task on IMDB data.
  - Large-Reach: Utilize bit word embeddings in neural machine translation.

* Progress and Preliminary Results
  - Downloaded and adapted Mikolov's CBOW code to use quantization and
    straight through estimator.
  - Reproduced embedding accuracy results of Mikolov's code on 100MB
    of Wikipedia data.
  - Verified that dropping all 23 bits of a float's mantissa has
    little to no effect on training on 100 MB of Wikipedia data (so at
    the very least we can show that 9 bits is sufficient to capture
    word relationships).
  - Verified that 400 dimensional, 2 bit word vectors matches the
    accuracy (~39-40%) attained by full precision vectors (with 200 dimensions)
    on 100 MB of Wikipedia data after training for 20 epochs. Expect
    that similar results will be achieved on full Wikipedia data.
  - Verified that using 1 bit word vectors with 200 dimensionality
    achieves ~20% accuracy on google's analogy data after training on
    100 MB of Wikipedia data for ~10 epochs. We expect better results
    after thoroughly tuning the dimensionality of the word vectors and
    quantization boundaries.
  - Verified that larger dimensionality (500 vs 200) with binary word
    vectors improves accuracy.

* Todo
  - Thoroughly tune performance of 1-bit, 2-bit, 4-bit, 8-bit
    embeddings (e.g: quantization boundary cutoffs, # of epochs to
    train, vector dimensionality) using 100 MB of Wikipedia data as
    dev set. Likely do exhaustive hyperparmater search with cluster
    after hand tuning a bit.
  - Optimize computational runtime of quantization code.
  - Train on full Wikipedia dataset and try to match state of the art results on analogy task.
  - Plot tradeoffs of training accuracy vs epochs vs amount of quantization
  - Plot tradeoffs of vector dimensionality vs amount of quantization vs accuracy attained
  - Evaluate 1-bit, 2-bit, 4-bit, 8-bit embeddings (after training on
    full wikipedia) on sentiment analysis task. Show accuracy matches
    that of using full precision embeddings.
  - Implement word embeddings parameter/vector shuffling algorithm and
    show that it yields highly compressible data. Evaluate compressed
    size of bit vectors vs regular word embeddings.
  - Reach goal: Evaluate 1-bit, 2-bit, 4-bit, 8-bit embeddings on
    neural machine translation.

* Related Work
  - https://arxiv.org/abs/1602.02830 (Binarized Neural Networks)
  - https://openreview.net/forum?id=BJRZzFlRb (Compressing Word Embeddings via Deep Compositional Code Learning) (We distinguish from them by directly learning the embeddings rather than training a neural network to compress word embeddings.)
