#+TITLE: CS224n Project Update: Word2Bits
#+LATEX_HEADER: \usepackage[a4paper,margin=3cm,footskip=.5cm]{geometry}
#+LATEX_HEADER: \usepackage{listings}
#+LaTeX_CLASS_OPTIONS: [microtype]
#+LaTeX_CLASS_OPTIONS: [10pt]
#+LATEX_HEADER: \usepackage{amsmath}
#+OPTIONS: toc:nil
#+AUTHOR: Maximilian Lam (maxlam)
#+DATE:

* Mentor
  Kevin Clark

* Problem Description
  Quantized word embeddings.

* Data
  Collected and processed 24 GB of Wikipedia data.

* Baseline
  Have run Mikolov's CBOW word2vec on 24 GB of Wikipedia data for 25 epochs
  - fullwiki bitlevel0 size200 window10 neg12 maxiter25 mincount5 out : 66% on analogy test
  - fullwiki bitlevel0 size400 window10 neg12 maxiter25 mincount5 out : 68% on analogy test
  800 dimension, full precision training in the pipeline.

* Experimental Results
  Have run 1bit, 2bit CBOW on 24 GB of Wikipedia data for 25 epochs.

  - fullwiki bitlevel2 size800 window10 neg12 maxiter25 mincount5 out : 68% on analogy test
  400 dimension, 2bit thresholded training in the pipeline.\\\\
  So far, 800 dimensional, 2 bit word embeddings performs on par with 400 dimensional, 32 bit word embeddings.

*  Evaluation Pipeline
   - Training pipeline set up
   - DrQa set up to evaluate word embeddings
   - Other intrinsic tasks set up (word similarity, msr, etc)

* Next Steps
  - Collect all trained word embeddings
  - Fully evaluate trained word embeddings on
    - All intrinsic tasks
      - MSR, Word similarity, etc
    - SQuAD extrinsic task
      - DrQA
  - Try to implement 8 bit quantized training and show it works.
